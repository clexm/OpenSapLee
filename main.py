#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import base64
import requests
import os
import sys
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ===================== æ ¸å¿ƒé…ç½® =====================
SUBSCRIBE_RAW_URLS = [
    "https://www.xrayvip.com/free.txt",
    "https://raw.githubusercontent.com/ermaozi/get_subscribe/main/subscribe/v2ray.txt",
    "https://raw.githubusercontent.com/aiboboxx/v2rayfree/main/v2",
    "https://raw.githubusercontent.com/Pawdroid/Free-servers/main/sub",
    "https://raw.githubusercontent.com/peasoft/NoMoreWalls/master/list.txt",
    "https://raw.githubusercontent.com/free-nodes/v2rayfree/main/v2",
    "https://raw.githubusercontent.com/free18/v2ray/refs/heads/main/v.txt",
    "http://59.33.33.157:18080/v2ray.txt",
    "https://raw.githubusercontent.com/shaoyouvip/free/main/base64.txt"
]

# ä»…è¿™ä¸ªé“¾æ¥çš„base64åé¢æœ‰æ—¶é—´æˆ³ï¼Œéœ€è¦æ¸…ç†
TARGET_CLEAN_URL = "https://raw.githubusercontent.com/shaoyouvip/free/main/base64.txt"
TIMEOUT = 15
RETRY_TIMES = 3

# ===================== å·¥å…·å‡½æ•° =====================
def init_request_session():
    session = requests.Session()
    retry_strategy = Retry(
        total=RETRY_TIMES,
        backoff_factor=0.5,
        allowed_methods=["GET"],
        status_forcelist=[429, 500, 502, 503, 504]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

def clean_base64_suffix(raw_str):
    """æ¸…ç†base64å­—ç¬¦ä¸²åé¢çš„ébase64å­—ç¬¦ï¼ˆæ—¶é—´æˆ³ã€æ³¨é‡Šç­‰ï¼‰"""
    valid_chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=')
    for i, char in enumerate(raw_str):
        if char not in valid_chars:
            print(f"âš ï¸  Found invalid char '{char}' at position {i} (target URL), truncating...")
            return raw_str[:i].strip()
    print("âœ… No invalid chars in target URL's base64 string")
    return raw_str.strip()

def download_and_decode_sub(session, url):
    try:
        print(f"\n=== Processing URL: {url} ===")
        # å‘é€è¯·æ±‚
        response = session.get(url, timeout=TIMEOUT, allow_redirects=True)
        response.raise_for_status()  # æŠ›å‡ºHTTPé”™è¯¯ï¼ˆ404ã€500ç­‰ï¼‰
        print(f"âœ… Request successful (status code: {response.status_code})")
        
        # è·å–åŸå§‹å†…å®¹
        raw_content = response.text.strip()
        print(f"Raw content length: {len(raw_content)} bytes")
        print(f"First 50 chars: {raw_content[:50]}...")
        
        if not raw_content:
            print("âŒ Raw content is empty!")
            return []
        
        # å…³é”®ï¼šä»…å¯¹ç›®æ ‡é“¾æ¥æ¸…ç†base64åç¼€ï¼ˆæ—¶é—´æˆ³ï¼‰
        if url == TARGET_CLEAN_URL:
            raw_content = clean_base64_suffix(raw_content)
        
        # è¡¥å…¨base64 paddingï¼ˆæ‰€æœ‰é“¾æ¥éƒ½éœ€è¦ï¼Œé¿å…è§£ç å¤±è´¥ï¼‰
        padding = len(raw_content) % 4
        if padding != 0:
            raw_content += "=" * (4 - padding)
            print(f"Added {4 - padding} padding chars for base64")
        
        # è§£ç base64ï¼ˆæ‰€æœ‰é“¾æ¥ç»Ÿä¸€å¤„ç†ï¼‰
        try:
            decoded_content = base64.b64decode(raw_content).decode("utf-8", errors="ignore")
            print(f"âœ… Base64 decoded successfully (decoded length: {len(decoded_content)} bytes)")
            print(f"Decoded first 100 chars: {decoded_content[:100]}...")
        except base64.binascii.Error as e:
            print(f"âŒ Base64 decode failed: {str(e)} â†’ Skipping this URL")
            return []
        
        # æå–èŠ‚ç‚¹ï¼ˆè¿‡æ»¤ç©ºè¡Œï¼Œæ”¯æŒæ¢è¡Œ/ç©ºæ ¼åˆ†éš”ï¼‰
        nodes = [node.strip() for node in decoded_content.split() if node.strip()]
        print(f"âœ… Extracted {len(nodes)} valid nodes from this URL")
        
        # è°ƒè¯•ï¼šè¾“å‡ºå‰2ä¸ªèŠ‚ç‚¹é¢„è§ˆ
        for i, node in enumerate(nodes[:2]):
            print(f"  Node {i+1}: {node[:60]}...")
        
        return nodes
    except requests.exceptions.RequestException as e:
        print(f"âŒ Request failed: {str(e)}", file=sys.stderr)
    except Exception as e:
        print(f"âŒ Unexpected error: {str(e)}", file=sys.stderr)
    return []

# ===================== æ ¸å¿ƒé€»è¾‘ =====================
def merge_all_subs():
    print("=" * 60)
    print("ğŸ”„ Starting subscription merge process (all base64 encoded)")
    print("=" * 60)
    
    # æ‰“å°å·¥ä½œç›®å½•ï¼ˆæ–¹ä¾¿æ’æŸ¥æ–‡ä»¶ä¿å­˜ä½ç½®ï¼‰
    print(f"\nğŸ“‚ Current working directory: {os.getcwd()}")
    script_dir = os.path.dirname(os.path.abspath(__file__))
    print(f"ğŸ“‚ Script directory: {script_dir}")
    
    session = init_request_session()
    all_nodes = []
    
    # éå†æ‰€æœ‰é“¾æ¥ï¼Œæå–èŠ‚ç‚¹
    for url in SUBSCRIBE_RAW_URLS:
        nodes = download_and_decode_sub(session, url)
        if nodes:
            all_nodes.extend(nodes)
            print(f"\nğŸ‰ Successfully added {len(nodes)} nodes from {url}")
    
    # ç»Ÿè®¡æ€»èŠ‚ç‚¹æ•°
    print("\n" + "=" * 60)
    print(f"ğŸ“Š Total valid nodes (after deduplication): {len(all_nodes)}")
    print("=" * 60)
    
    # æ— èŠ‚ç‚¹æ—¶é€€å‡º
    if not all_nodes:
        print("\nâŒ No valid nodes found in any URL! Check debug output for details.")
        return
    
    # ä¿å­˜åˆå¹¶åçš„base64æ–‡ä»¶
    merged_text = "\n".join(all_nodes)
    final_base64 = base64.b64encode(merged_text.encode("utf-8")).decode("utf-8")
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    script_dir = os.path.dirname(os.path.abspath(__file__))
    date_dir = os.path.join(script_dir, "Date")
    os.makedirs(date_dir, exist_ok=True)
    output_file = os.path.join(date_dir, "List.txt")
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(final_base64)
    
    # æ‰“å°ç»“æœ
    file_size = os.path.getsize(output_file)
    print(f"\nâœ… File saved successfully!")
    print(f"ğŸ“ Path: {output_file}")
    print(f"ğŸ“ Size: {file_size} bytes")
    print(f"ğŸ” Final base64 first 50 chars: {final_base64[:50]}...")

if __name__ == "__main__":
    merge_all_subs()
